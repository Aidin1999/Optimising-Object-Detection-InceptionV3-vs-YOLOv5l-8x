{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKnHwoNx2YgB"
   },
   "source": [
    "### Transitioning from Custom Model to YOLOv8/YOLOv5 on High-Performance Infrastructure\n",
    "\n",
    "After successfully building and training a **custom object detection model** using an InceptionV3 backbone, we implemented core components including:\n",
    "\n",
    "- Data preprocessing and label transformation  \n",
    "- Grid-based YOLO tensor encoding  \n",
    "- Custom loss function with IoU-based penalty  \n",
    "- Manual anchor assignment heuristics  \n",
    "- Inference with bounding box decoding and Non-Maximum Suppression (NMS)  \n",
    "- **Selective layer freezing (only `mixed7` was trainable)**  \n",
    "- **Learning rate decay using `ExponentialDecay` scheduler**\n",
    "\n",
    "This custom pipeline provided deep insights into how YOLO-style detection works at a granular level. It gave full control over feature extraction, model capacity, and training behaviour. However, it was **computationally expensive** and required careful manual tuning to achieve even modest performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Why We Transitioned to YOLOv8 and YOLOv5\n",
    "\n",
    "To benchmark performance and streamline experimentation, we transitioned to **YOLOv8** and **YOLOv5** models using **GPU-enabled infrastructure** (e.g., Google Cloud). These models offer:\n",
    "\n",
    "- **Superior accuracy out of the box**\n",
    "- **Faster training and inference** via highly optimised pipelines\n",
    "- **Minimal setup** — no need to define losses, backbones, or training loops\n",
    "- Built-in support for:\n",
    "  - Anchor-free and anchor-based detection  \n",
    "  - Data augmentation  \n",
    "  - Post-processing (NMS, confidence thresholds, etc.)\n",
    "\n",
    "Unlike our custom setup, these models allowed us to skip architecture design, preprocessing pipelines, and manual loss handling — drastically speeding up the training-validation cycle.\n",
    "\n",
    "---\n",
    "\n",
    "> - This experience highlights a common engineering approach:\n",
    "> - Start **custom** to understand the inner workings and design choices  \n",
    "> - Then move to **robust, production-ready tools** when efficiency and scalability become priorities\n",
    "\n",
    "---\n",
    "\n",
    "We began the transition by installing the `ultralytics` package:\n",
    "```bash\n",
    "pip install ultralytics\n",
    "```\n",
    "This enabled us to load and evaluate YOLOv8 and YOLOv5 models easily using their `.predict()` API and visualise results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd5L9IdtSoVL",
    "outputId": "3f39face-3d35-408d-8999-27c9349f2f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.168-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Downloading ultralytics-8.3.168-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.168 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMtcohWH3sfx"
   },
   "source": [
    "### Dataset Preparation for YOLO Training\n",
    "\n",
    "This section unpacks the dataset ZIP archive and prepares the folder structure required by Ultralytics YOLOv8:\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Unzip the Dataset\n",
    "- The dataset (in YOLO format) is downloaded from Google Drive as a `.zip` file.\n",
    "- It is extracted to a folder next to the ZIP location using Python's `zipfile` module.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Define and Create Dataset Paths\n",
    "- A working directory `yolo_dataset/` is created within the extracted dataset folder.\n",
    "- This will hold the **training/validation images and labels** in the structure required by YOLO.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Directory Structure Setup\n",
    "\n",
    "YOLO expects the following structure:\n",
    "\n",
    "```kotlin\n",
    "yolo_dataset/\n",
    "│\n",
    "├── images/\n",
    "│ ├── train/\n",
    "│ └── val/\n",
    "│\n",
    "└── labels/\n",
    "├── train/\n",
    "└── val/\n",
    "```\n",
    "\n",
    "Each image should have a corresponding `.txt` file in the `labels/` folder with the same filename, containing class and bounding box information.\n",
    "\n",
    "This setup ensures compatibility with YOLOv8’s built-in training functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMDh0FN5HNfB"
   },
   "outputs": [],
   "source": [
    "import os                                              # For file and directory operations\n",
    "import zipfile                                         # To handle .zip file extraction\n",
    "import shutil                                          # For file/folder copying and moving\n",
    "import glob                                            # For pattern matching file paths (e.g., *.jpg)\n",
    "from sklearn.model_selection import train_test_split   # For splitting dataset into train/val\n",
    "from ultralytics import YOLO                           # Import YOLO (Ultralytics) for training/detection\n",
    "\n",
    "# === Step 1: Unzip your dataset ===\n",
    "\n",
    "# Define the path to your zipped dataset (assumed stored in Google Drive)\n",
    "ZIP_PATH = r\"/content/drive/MyDrive/People Detection -General-.v8i.darknet.zip\"\n",
    "\n",
    "# Define the folder where the zip will be extracted.\n",
    "# This joins the zip's parent directory with the intended output folder.\n",
    "EXTRACTED_PATH = os.path.join(os.path.dirname(ZIP_PATH), \"People Detection -General-.v8i.darknet\")\n",
    "\n",
    "# Open the zip file and extract all contents to the specified directory\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(EXTRACTED_PATH)\n",
    "\n",
    "\n",
    "# === Step 2: Define paths ===\n",
    "# Set the base path to the extracted dataset directory\n",
    "BASE_PATH = EXTRACTED_PATH\n",
    "\n",
    "# Define a subdirectory to store processed data or outputs\n",
    "DATASET_PATH = os.path.join(BASE_PATH, \"yolo_dataset\")\n",
    "\n",
    "# Create the yolo_dataset directory if it doesn't exist already\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# === Step 3: Prepare directory structure ===\n",
    "# Create folders for training and validation sets for both images and labels\n",
    "# Required by YOLO format: one .txt file per image with matching filename\n",
    "for subfolder in [\"images/train\", \"images/val\", \"labels/train\", \"labels/val\"]:\n",
    "    os.makedirs(os.path.join(DATASET_PATH, subfolder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU3ISGkw4bzb"
   },
   "source": [
    "### Organise Dataset and Create YOLO Config File (`people.yaml`)\n",
    "\n",
    "This section prepares the dataset for training with Ultralytics YOLOv8 by performing two main tasks:\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Organise Images and Labels\n",
    "\n",
    "- All `.jpg` images are collected from:\n",
    "  - `train/` → moved to `images/train/`\n",
    "  - `test/` → moved to `images/val/`\n",
    "- Each image’s corresponding `.txt` label file (in YOLO format) is also copied to:\n",
    "  - `labels/train/` or `labels/val/`\n",
    "- Each label must match the image filename (e.g., `dog.jpg` ↔ `dog.txt`) and contain lines in the format:\n",
    "\n",
    "```php-template\n",
    "<class_id> <x_center> <y_center> <width> <height>\n",
    "```\n",
    "\n",
    "with all values normalised to `[0, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📄 Step 5: Create the `people.yaml` File\n",
    "\n",
    "This file is required by YOLOv8 for training and specifies:\n",
    "- The root `path` to the dataset\n",
    "- Relative paths to the training/validation images\n",
    "- `nc`: Number of object classes (`1`)\n",
    "- `names`: List of class names (`['person']`)\n",
    "\n",
    "Example content:\n",
    "```yaml\n",
    "path: /content/People Detection -General-.v8i.darknet/yolo_dataset\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 1\n",
    "names: ['person']\n",
    "```\n",
    "\n",
    "This completes the YOLO-compatible dataset setup, ready for training using:\n",
    "\n",
    "``` python\n",
    "YOLO(\"yolov8x.yaml\").train(data=\"people.yaml\", epochs=...)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ3i4D-OLPtw"
   },
   "outputs": [],
   "source": [
    "# === Step 4: Move images and labels ===\n",
    "\n",
    "# Get all training image paths (*.jpg) from 'train' folder inside the extracted dataset\n",
    "train_imgs = glob.glob(os.path.join(BASE_PATH, \"train\", \"*.jpg\"))\n",
    "\n",
    "# Get all validation image paths (*.jpg) from 'test' folder inside the extracted dataset\n",
    "val_imgs = glob.glob(os.path.join(BASE_PATH, \"test\", \"*.jpg\"))\n",
    "\n",
    "# Define a function to move images and their matching label files to the correct destination\n",
    "def move_files(img_list, subset):\n",
    "    for img_path in img_list:\n",
    "        label_path = img_path.replace(\".jpg\", \".txt\")  # Assumes label filename matches image\n",
    "        img_name = os.path.basename(img_path)          # Extracts just the filename from path\n",
    "        label_name = os.path.basename(label_path)\n",
    "\n",
    "        # Copy image to the images/<subset>/ folder\n",
    "        shutil.copy(img_path, os.path.join(DATASET_PATH, f\"images/{subset}\", img_name))\n",
    "\n",
    "        # If a label file exists for this image, copy it to labels/<subset>/\n",
    "        if os.path.exists(label_path):\n",
    "            shutil.copy(label_path, os.path.join(DATASET_PATH, f\"labels/{subset}\", label_name))\n",
    "\n",
    "# Move training images and labels to the correct folders\n",
    "move_files(train_imgs, \"train\")\n",
    "\n",
    "# Move validation images and labels to the correct folders\n",
    "move_files(val_imgs, \"val\")\n",
    "\n",
    "\n",
    "# === Step 5: Create people.yaml file ===\n",
    "\n",
    "# Convert backslashes to forward slashes in path (for compatibility across OS)\n",
    "yaml_path_clean = DATASET_PATH.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define the content of the YOLO dataset configuration file\n",
    "# - path: root directory of the dataset\n",
    "# - train: relative path to training images\n",
    "# - val: relative path to validation images\n",
    "# - nc: number of object classes (1 in this case)\n",
    "# - names: list of class names\n",
    "yaml_content = f\"\"\"\n",
    "path: {yaml_path_clean}\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 1\n",
    "names: ['person']\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML configuration to a file called 'people.yaml' in the dataset folder\n",
    "with open(os.path.join(DATASET_PATH, \"people.yaml\"), \"w\") as f:\n",
    "    f.write(yaml_content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVBnzCHY6Nth"
   },
   "source": [
    "### Quick Training and Inference with YOLOv5 (Low Infrastructure)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6: Lightweight YOLOv5 Training\n",
    "\n",
    "This step demonstrates how to train a **YOLOv5 model** on a custom dataset using **limited computational resources** (e.g., CPU or a small GPU):\n",
    "\n",
    "- Pretrained model: `yolov5l.pt` (YOLOv5 Large)\n",
    "- Training config:\n",
    "  - **Epochs**: 5 (quick training pass)\n",
    "  - **Image size**: 640×640 (standard resolution)\n",
    "  - **Batch size**: 16 (adjustable for GPU/VRAM limits)\n",
    "  - **Caching**: Disabled to conserve RAM\n",
    "  - **Workers**: 8 threads for efficient data loading\n",
    "- This setup is ideal for fast iteration, testing pipelines, and training on shared or free resources (e.g., Google Colab with T4 GPUs)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 7: Test Prediction\n",
    "\n",
    "After training, the model performs inference on a sample image from the validation set:\n",
    "\n",
    "- The result is **automatically saved** to the `runs/detect/predict/` folder.\n",
    "- If `show=True`, the image is also rendered in a local pop-up window (if supported).\n",
    "\n",
    "---\n",
    "\n",
    "> YOLOv5 offers strong accuracy and fast convergence even in low-epoch, low-resource settings — making it a great tool for rapid experimentation before scaling up training on YOLOv8 or larger models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyK7Pfzn6Oua",
    "outputId": "dc444a36-e44d-4121-8007-c45c8e97fae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP 💡 Replace 'model=yolov5l.pt' with new 'model=yolov5lu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5lu.pt to 'yolov5lu.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102M/102M [00:00<00:00, 287MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.167 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/people.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov5l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      7040  ultralytics.nn.modules.conv.Conv             [3, 64, 6, 2, 2]              \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   1118208  ultralytics.nn.modules.block.C3              [256, 256, 6]                 \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  9   6433792  ultralytics.nn.modules.block.C3              [512, 512, 9]                 \n",
      "  7                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n",
      "  8                  -1  3   9971712  ultralytics.nn.modules.block.C3              [1024, 1024, 3]               \n",
      "  9                  -1  1   2624512  ultralytics.nn.modules.block.SPPF            [1024, 1024, 5]               \n",
      " 10                  -1  1    525312  ultralytics.nn.modules.conv.Conv             [1024, 512, 1, 1]             \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  3   2757632  ultralytics.nn.modules.block.C3              [1024, 512, 3, False]         \n",
      " 14                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  3    690688  ultralytics.nn.modules.block.C3              [512, 256, 3, False]          \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  3   2495488  ultralytics.nn.modules.block.C3              [512, 512, 3, False]          \n",
      " 21                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  3   9971712  ultralytics.nn.modules.block.C3              [1024, 1024, 3, False]        \n",
      " 24        [17, 20, 23]  1   7058131  ultralytics.nn.modules.head.Detect           [1, [256, 512, 1024]]         \n",
      "YOLOv5l summary: 241 layers, 53,164,115 parameters, 53,164,099 gradients, 135.3 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 1.5±1.6 ms, read: 24.3±14.2 MB/s, size: 44.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/labels/train.cache... 16195 images, 2746 backgrounds, 0 corrupt: 100%|██████████| 16195/16195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 2.2±3.3 ms, read: 26.3±26.5 MB/s, size: 65.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/labels/val.cache... 1897 images, 3 backgrounds, 0 corrupt: 100%|██████████| 1897/1897 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      9.57G       1.58      1.579      1.526         33        640: 100%|██████████| 1013/1013 [13:44<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:38<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.673      0.539      0.615      0.255\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5       9.8G      1.575      1.614      1.538         15        640: 100%|██████████| 1013/1013 [13:15<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:36<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.681      0.589      0.677      0.323\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      9.71G      1.497      1.501      1.479         12        640: 100%|██████████| 1013/1013 [13:00<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:37<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.702      0.658      0.679      0.255\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      9.93G      1.419      1.396      1.429         11        640: 100%|██████████| 1013/1013 [12:58<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:37<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.753      0.711      0.769      0.359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      9.83G      1.334      1.267      1.364          2        640: 100%|██████████| 1013/1013 [12:55<00:00,  1.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.776       0.75      0.793      0.369\n",
      "\n",
      "5 epochs completed in 1.167 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 106.8MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 106.8MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics 8.3.167 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "YOLOv5l summary (fused): 128 layers, 53,132,179 parameters, 0 gradients, 134.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:40<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.773       0.75      0.793      0.369\n",
      "Speed: 0.2ms preprocess, 15.3ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
      "\n",
      "image 1/1 /content/drive/MyDrive/People Detection -General-.v8i.darknet/test/005304_jpg.rf.19efca555ace117390f5a1e9761e630a.jpg: 640x640 2 persons, 43.6ms\n",
      "Speed: 2.2ms preprocess, 43.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/train42\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 6: Train the YOLOv5 model with low infrastructure (fast & inexpensive) ===\n",
    "\n",
    "# Load a pre-trained YOLOv5 'large' model (you can also try 's' or 'n' for lighter versions)\n",
    "model = YOLO(\"yolov5l.pt\")\n",
    "\n",
    "# Train the model on your custom dataset\n",
    "model.train(\n",
    "    data=os.path.join(DATASET_PATH, \"people.yaml\").replace(\"\\\\\", \"/\"),  # Path to dataset config\n",
    "    epochs=5,               # Low number of epochs for quick training\n",
    "    imgsz=640,              # Image resolution (balanced between speed and accuracy)\n",
    "    batch=16,               # Batch size (adjust based on available VRAM)\n",
    "    cache=False,            # Avoid caching images to save memory\n",
    "    device=0,               # Use GPU 0 (if available), or set to 'cpu' for CPU\n",
    "    workers=8,              # Number of parallel data loader threads\n",
    ")\n",
    "\n",
    "# === Step 7: Predict on a test image ===\n",
    "TEST_IMAGE = val_imgs[0]  # Predict on the first image from test set\n",
    "results = model.predict(source=TEST_IMAGE, save=True, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l_1Hv2iCHTj"
   },
   "source": [
    "### YOLOv5lu (Ultralytics Upgraded) – Training Summary on Google Cloud GPU\n",
    "\n",
    "We trained the upgraded **YOLOv5l-u model (`yolov5lu.pt`)** using **Ultralytics 8.3+**, which offers better performance than the legacy YOLOv5 from the original repo.\n",
    "\n",
    "> **PRO TIP**\n",
    "> YOLOv5-u models (`yolov5lu.pt`) are pretrained via the **Ultralytics v8 engine**, improving training convergence, mAP, and runtime stability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Configuration:\n",
    "\n",
    "- **Model**: `yolov5lu.pt` (YOLOv5 Large, upgraded)\n",
    "- **Epochs**: 5\n",
    "- **Image size**: 640×640\n",
    "- **Batch size**: 16\n",
    "- **Optimizer**: Automatically selected (AdamW)\n",
    "- **GPU**: Tesla T4 (15 GB memory)\n",
    "- **Total Training Time**: ~1.17 hours\n",
    "\n",
    "---\n",
    "\n",
    "#### Performance Summary (Epoch 5/5):\n",
    "\n",
    "| Metric       | Value     |\n",
    "|--------------|-----------|\n",
    "| Precision    | **0.776** |\n",
    "| Recall       | **0.750** |\n",
    "| mAP@0.5      | **0.793** |\n",
    "| mAP@0.5:0.95 | **0.369** |\n",
    "\n",
    "> The model shows **high detection performance** with very good **box precision and class confidence** after just 5 epochs — validating YOLOv5's efficiency on high-quality datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Inference Speed (per image):\n",
    "\n",
    "- **Preprocess**: 2.2 ms  \n",
    "- **Inference**: 43.6 ms  \n",
    "- **Postprocess**: 1.9 ms  \n",
    "- **Total**: ~47.7 ms per image at 640×640\n",
    "\n",
    "---\n",
    "\n",
    "#### Outputs:\n",
    "\n",
    "- Trained model saved to: `runs/detect/train4/weights/best.pt`\n",
    "- Annotated predictions: `runs/detect/train42/`\n",
    "- Labels plot: `runs/detect/train4/labels.jpg`\n",
    "\n",
    "---\n",
    "\n",
    "### Insight:\n",
    "By switching to `yolov5lu.pt`, we leveraged the latest training pipeline under Ultralytics v8, which simplifies training and boosts performance **without needing any custom architecture design** or learning rate tuning. This makes it an ideal choice for deployment-ready pipelines on GPU-based infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etEc8H2iEzIe"
   },
   "source": [
    "### Step 6: Training the YOLOv8x Supermodel with High-End Infrastructure\n",
    "\n",
    "We upgraded to **YOLOv8x** — the **largest and most accurate version** available from Ultralytics — to take full advantage of high-performance GPU resources.\n",
    "\n",
    "Compared to our custom Inception-based model and lighter YOLO variants, YOLOv8x provides:\n",
    "\n",
    "- **State-of-the-art accuracy** with deeper backbone and advanced architecture  \n",
    "- **Faster convergence** using optimised pipelines and mixed precision (AMP)  \n",
    "- **Automatic post-processing** (IoU-based NMS, confidence filtering)  \n",
    "- **Simplified training workflow** requiring minimal manual engineering\n",
    "\n",
    "> Ideal for final-stage model deployment or benchmarking on powerful hardware (e.g., A100).\n",
    "\n",
    "#### 🔧 Training Configuration Summary:\n",
    "\n",
    "| Parameter       | Value                          |\n",
    "|------------------|-------------------------------|\n",
    "| **Model**        | `yolov8x.pt` (Ultralytics)     |\n",
    "| **Epochs**       | 20                             |\n",
    "| **Image Size**   | 640 × 640                      |\n",
    "| **Batch Size**   | 16                             |\n",
    "| **Device**       | `GPU:0` (CUDA-enabled)         |\n",
    "| **Cache**        | Enabled (for faster access)    |\n",
    "| **Workers**      | 16                             |\n",
    "| **Dataset**      | `people.yaml` (custom dataset) |\n",
    "\n",
    "#### Learning Improvements vs Previous Models:\n",
    "\n",
    "- **No manual loss engineering** (vs custom YOLO-Inception)\n",
    "- **Higher precision and recall** (vs YOLOv5 on same data)\n",
    "- **Time-efficient**, even on large datasets, using `ultralytics` pre-built pipeline\n",
    "\n",
    "---\n",
    "\n",
    "Training this version is especially valuable for **production use cases** or **benchmarking** against academic or business-grade object detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yb0AshzcZNu",
    "outputId": "477df95a-029c-4fb9-fd43-eb4819a5a5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/people.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=16, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,153,571 parameters, 68,153,555 gradients, 258.1 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 20.5±13.0 MB/s, size: 44.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/labels/train.cache... 16195 images, 2746 backgrounds, 0 corrupt: 100%|██████████| 16195/16195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0m27.8GB RAM required to cache images with 50% safety margin but only 23.8/83.5GB available, not caching images\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 1.1±1.1 ms, read: 7.4±7.5 MB/s, size: 67.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/People Detection -General-.v8i.darknet/yolo_dataset/labels/val.cache... 1897 images, 3 backgrounds, 0 corrupt: 100%|██████████| 1897/1897 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.2GB RAM): 100%|██████████| 1897/1897 [00:03<00:00, 519.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 12 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      30.9G      1.572      1.608      1.555         27        640: 100%|██████████| 1013/1013 [03:53<00:00,  4.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:13<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.607      0.572       0.54      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      31.5G      1.565        1.6      1.543          9        640: 100%|██████████| 1013/1013 [03:42<00:00,  4.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.684      0.646      0.673       0.27\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      31.5G      1.497      1.498       1.49         33        640: 100%|██████████| 1013/1013 [03:39<00:00,  4.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.732      0.639      0.702      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      31.7G      1.435      1.397      1.444         15        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.723      0.736      0.762      0.363\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      31.8G      1.387      1.342      1.414          5        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.773      0.718      0.747      0.344\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      31.5G      1.344      1.263      1.384          9        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.764      0.753      0.778      0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      31.8G       1.32      1.217      1.362          8        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.768      0.752      0.793      0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      31.8G      1.297      1.186      1.356         12        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.771      0.788      0.803      0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      31.7G      1.272      1.143      1.333         27        640: 100%|██████████| 1013/1013 [03:37<00:00,  4.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.769      0.768      0.806      0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      31.5G      1.247      1.113       1.32         30        640: 100%|██████████| 1013/1013 [03:36<00:00,  4.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.761      0.791      0.818      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      31.8G       1.27     0.9869      1.337          3        640: 100%|██████████| 1013/1013 [03:50<00:00,  4.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731       0.78      0.791      0.821      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      31.7G      1.243     0.9652      1.328          2        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.772      0.808      0.817      0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      31.7G      1.209     0.9197      1.304         13        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.769      0.832      0.841      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      31.5G       1.18     0.8911      1.283          1        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.801      0.806      0.853      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      31.8G      1.155     0.8447      1.267          2        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.786      0.815      0.837      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      31.8G      1.133      0.814      1.247          4        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.794      0.804      0.848      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      31.7G      1.099     0.7896      1.231          3        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.789      0.828      0.864      0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      31.5G      1.067     0.7561      1.211          6        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.793      0.824      0.857       0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      31.8G      1.034     0.7267      1.195          4        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.783      0.837       0.86       0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      31.7G      1.011     0.6932      1.173          3        640: 100%|██████████| 1013/1013 [03:35<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:12<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731      0.791      0.832      0.854      0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 1.302 hours.\n",
      "Optimizer stripped from runs/detect/train5/weights/last.pt, 136.7MB\n",
      "Optimizer stripped from runs/detect/train5/weights/best.pt, 136.7MB\n",
      "\n",
      "Validating runs/detect/train5/weights/best.pt...\n",
      "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
      "Model summary (fused): 112 layers, 68,124,531 parameters, 0 gradients, 257.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [00:13<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1897       4731       0.79      0.833      0.854      0.585\n",
      "Speed: 0.1ms preprocess, 3.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7efd30fb2810>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,\n",
       "            0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.99254,     0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98315,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98031,\n",
       "            0.98031,     0.98031,     0.98031,     0.98031,     0.98031,     0.98031,     0.98031,     0.97857,     0.97857,     0.97857,     0.97857,     0.97857,     0.97288,     0.97288,     0.97288,      0.9702,     0.96815,     0.96815,     0.96815,     0.96573,     0.96319,     0.96096,     0.95858,\n",
       "            0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,\n",
       "            0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95851,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95848,     0.95782,     0.95782,\n",
       "            0.95782,     0.95574,     0.95574,     0.95574,     0.95574,     0.95574,     0.95574,     0.95574,     0.95574,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95509,     0.95104,     0.95087,     0.95087,\n",
       "            0.95087,     0.95077,     0.95077,     0.95077,     0.95077,     0.94723,     0.94723,     0.94723,     0.94723,     0.94723,     0.94723,     0.94723,     0.94723,     0.94723,      0.9471,      0.9471,      0.9471,     0.94624,     0.94624,     0.94624,     0.94624,     0.94624,     0.94624,\n",
       "            0.94624,     0.94624,     0.94624,     0.94624,     0.94624,     0.94624,     0.94624,     0.94458,     0.94458,     0.94406,     0.94406,     0.94342,     0.94248,     0.94248,     0.94248,     0.94248,     0.94248,     0.94248,     0.94248,     0.94243,     0.94243,     0.94243,     0.94243,\n",
       "            0.94243,     0.94243,     0.94243,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,\n",
       "            0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94184,     0.94182,     0.94182,     0.94182,     0.94182,     0.94182,     0.94182,     0.94149,     0.94149,     0.94075,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,\n",
       "               0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,        0.94,\n",
       "               0.94,        0.94,        0.94,        0.94,        0.94,        0.94,     0.93907,     0.93907,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93869,     0.93853,     0.93853,     0.93853,     0.93853,\n",
       "            0.93853,     0.93853,     0.93808,     0.93626,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93601,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,\n",
       "            0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93585,     0.93567,     0.93567,     0.93466,     0.93337,     0.93337,     0.93337,     0.93337,\n",
       "            0.93337,     0.93265,     0.93265,     0.93237,     0.93237,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93186,     0.93148,     0.93109,     0.93056,     0.93056,     0.93056,     0.93056,     0.93056,\n",
       "            0.93056,     0.93056,     0.93056,     0.93056,     0.93056,     0.93056,     0.93056,     0.93056,     0.93056,     0.93043,     0.93043,     0.93014,     0.93014,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,\n",
       "            0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92973,     0.92936,     0.92887,     0.92887,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,\n",
       "            0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92857,     0.92847,     0.92847,     0.92779,     0.92705,\n",
       "            0.92666,     0.92666,     0.92666,     0.92639,     0.92562,     0.92562,     0.92562,     0.92546,     0.92546,      0.9254,      0.9254,      0.9254,      0.9254,     0.92532,     0.92532,     0.92532,     0.92511,     0.92511,     0.92511,     0.92511,     0.92511,     0.92511,     0.92493,\n",
       "            0.92493,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92458,     0.92431,     0.92431,     0.92431,     0.92431,     0.92431,     0.92431,     0.92431,     0.92431,     0.92431,\n",
       "            0.92431,     0.92431,     0.92379,     0.92379,     0.92379,     0.92379,     0.92359,     0.92359,     0.92337,     0.92333,     0.92333,     0.92333,     0.92333,     0.92298,     0.92264,     0.92264,     0.92264,     0.92264,     0.92264,     0.92264,     0.92264,     0.92264,     0.92264,\n",
       "            0.92264,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,     0.92263,\n",
       "            0.92263,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92169,     0.92143,     0.92126,     0.92097,     0.92045,     0.92045,     0.92045,     0.91988,     0.91969,     0.91934,     0.91934,     0.91934,      0.9186,\n",
       "             0.9186,      0.9186,      0.9186,      0.9186,      0.9186,      0.9186,      0.9186,      0.9186,      0.9184,      0.9184,      0.9184,     0.91783,     0.91765,     0.91759,     0.91759,      0.9172,      0.9172,     0.91649,     0.91587,     0.91568,     0.91568,     0.91568,     0.91568,\n",
       "            0.91568,     0.91568,     0.91568,     0.91539,     0.91352,     0.91352,     0.91348,     0.91348,     0.91309,     0.91309,     0.91295,     0.91248,     0.91232,     0.91178,     0.91144,     0.91144,     0.91081,     0.91081,     0.91062,      0.9105,     0.91033,     0.91033,     0.91033,\n",
       "            0.91033,     0.91033,     0.91027,     0.91027,     0.90954,     0.90954,     0.90954,     0.90933,     0.90885,     0.90885,     0.90885,     0.90885,     0.90885,     0.90808,     0.90797,     0.90797,     0.90797,     0.90783,     0.90759,     0.90748,     0.90672,     0.90651,     0.90651,\n",
       "            0.90651,     0.90651,     0.90631,     0.90559,     0.90526,     0.90526,     0.90457,     0.90457,     0.90457,     0.90457,     0.90457,     0.90457,     0.90457,     0.90442,     0.90442,     0.90442,     0.90401,     0.90354,     0.90341,     0.90341,     0.90341,     0.90341,     0.90341,\n",
       "            0.90304,      0.9026,     0.90245,     0.90177,     0.90094,     0.90094,     0.90085,     0.90018,     0.90003,     0.89964,     0.89814,     0.89799,     0.89734,     0.89722,     0.89577,     0.89475,     0.89475,     0.89475,     0.89475,     0.89475,     0.89461,     0.89461,     0.89461,\n",
       "            0.89461,     0.89461,      0.8946,     0.89452,     0.89452,     0.89419,     0.89338,     0.89338,     0.89338,     0.89325,     0.89324,     0.89324,     0.89285,       0.892,       0.892,       0.892,     0.89111,      0.8911,      0.8904,     0.89034,     0.88996,     0.88908,     0.88801,\n",
       "            0.88792,     0.88733,     0.88641,     0.88641,     0.88632,     0.88605,     0.88605,     0.88547,      0.8852,     0.88481,     0.88347,     0.88315,     0.88243,     0.88201,     0.88173,     0.88158,     0.88085,     0.88026,     0.87961,     0.87936,     0.87878,      0.8785,     0.87786,\n",
       "            0.87772,     0.87772,     0.87757,     0.87757,     0.87683,     0.87683,     0.87663,     0.87653,     0.87533,       0.875,      0.8746,      0.8746,     0.87444,     0.87425,     0.87346,     0.87294,     0.87285,     0.87285,     0.87211,     0.87134,     0.87131,     0.87118,     0.87097,\n",
       "            0.87081,     0.86896,      0.8672,     0.86696,     0.86677,     0.86619,     0.86619,     0.86535,     0.86535,     0.86498,     0.86465,     0.86361,     0.86346,     0.86277,     0.86251,     0.86243,     0.86242,     0.86191,     0.86077,     0.86044,     0.86026,      0.8594,      0.8589,\n",
       "            0.85862,     0.85795,     0.85788,     0.85557,     0.85533,     0.85422,     0.85395,     0.85316,     0.85224,     0.85178,     0.85178,     0.85178,     0.85114,     0.85084,     0.85041,     0.85035,     0.84931,     0.84848,     0.84701,     0.84619,     0.84465,     0.84452,     0.84335,\n",
       "            0.84334,     0.84211,     0.84118,      0.8405,      0.8405,     0.83929,     0.83912,     0.83883,     0.83883,     0.83883,      0.8382,     0.83743,     0.83715,      0.8368,      0.8368,     0.83633,     0.83633,      0.8361,      0.8356,     0.83496,     0.83454,      0.8345,      0.8345,\n",
       "             0.8342,     0.83408,     0.83345,      0.8327,     0.83177,      0.8294,     0.82823,     0.82714,     0.82663,     0.82584,     0.82421,     0.82332,      0.8228,     0.82205,     0.82171,     0.82065,      0.8199,     0.81962,     0.81907,     0.81895,     0.81844,     0.81771,     0.81717,\n",
       "            0.81596,     0.81572,     0.81527,     0.81495,      0.8129,     0.81224,     0.81135,     0.81122,     0.81099,     0.81038,     0.80956,     0.80689,     0.80557,     0.80557,     0.80485,     0.80397,     0.80285,     0.80053,     0.79877,     0.79625,     0.79572,     0.79533,     0.79461,\n",
       "            0.79425,     0.79425,      0.7921,      0.7912,     0.79034,     0.78983,     0.78895,     0.78895,      0.7881,     0.78717,     0.78598,     0.78538,     0.78435,     0.78421,     0.78319,     0.78219,     0.78219,     0.78153,     0.78079,     0.78028,      0.7786,      0.7767,     0.77383,\n",
       "            0.77311,     0.77269,     0.77055,     0.76974,     0.76748,     0.76577,     0.76527,      0.7619,     0.76137,     0.75876,     0.75691,     0.75667,     0.75662,     0.75554,     0.75243,      0.7516,     0.74995,     0.74895,     0.74823,     0.74525,     0.74418,     0.74227,     0.74069,\n",
       "            0.73968,     0.73697,     0.73551,      0.7334,     0.73157,     0.73027,     0.72854,     0.72705,     0.72618,     0.72283,     0.72152,     0.71978,     0.71702,      0.7147,     0.71228,      0.7112,     0.71055,     0.70841,     0.70522,     0.69998,     0.69845,     0.69743,     0.69483,\n",
       "            0.69253,     0.69087,     0.68711,     0.68526,     0.68196,     0.67724,     0.67418,     0.67179,     0.66885,     0.66404,     0.66052,     0.65976,     0.65896,     0.65751,     0.65399,     0.64821,     0.64634,     0.64286,      0.6398,       0.637,     0.63541,     0.62951,     0.62814,\n",
       "            0.62592,     0.61855,       0.614,     0.60874,     0.60384,     0.60113,     0.59804,     0.58664,     0.58281,     0.57667,     0.57023,      0.5678,     0.55803,     0.55314,     0.54766,     0.54218,     0.53694,     0.53412,     0.53019,     0.52373,      0.5184,      0.5078,     0.50321,\n",
       "            0.48889,     0.48508,     0.48119,     0.47035,     0.46763,     0.46336,     0.45131,     0.45017,     0.44528,     0.43202,     0.42643,     0.42438,     0.41545,     0.39993,     0.38707,     0.36835,     0.35418,      0.3505,     0.33724,     0.32716,     0.31733,     0.30745,     0.29329,\n",
       "            0.28743,      0.2757,     0.25263,     0.23935,     0.22222,     0.20787,     0.19924,     0.19165,       0.187,     0.16699,      0.1633,     0.15771,     0.15221,     0.13267,     0.11902,     0.10458,    0.084518,    0.080114,     0.07254,    0.063874,    0.059312,    0.054749,    0.050187,\n",
       "           0.045624,    0.041062,    0.036499,    0.031937,    0.027375,    0.022812,     0.01825,    0.013687,   0.0091249,   0.0045624,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.12792,     0.12792,     0.17945,     0.21829,     0.24882,     0.27491,     0.29697,     0.31539,     0.33188,     0.34674,     0.36055,     0.37386,     0.38572,     0.39558,     0.40603,     0.41524,     0.42416,     0.43286,     0.44042,     0.44817,     0.45515,     0.46176,     0.46813,\n",
       "            0.47375,     0.48051,     0.48563,     0.49172,     0.49692,     0.50194,     0.50702,     0.51147,      0.5162,     0.52066,     0.52453,     0.52873,     0.53226,     0.53604,     0.53979,     0.54428,     0.54795,     0.55142,     0.55393,      0.5569,     0.56064,      0.5639,     0.56613,\n",
       "            0.56901,       0.571,     0.57388,      0.5771,     0.57952,      0.5812,     0.58372,     0.58582,     0.58827,     0.59025,     0.59229,     0.59395,     0.59571,     0.59785,     0.59964,      0.6016,     0.60354,     0.60567,     0.60761,     0.60903,     0.61048,     0.61205,     0.61393,\n",
       "            0.61549,     0.61703,     0.61903,     0.62069,     0.62269,     0.62441,      0.6259,     0.62659,     0.62773,     0.62975,     0.63125,     0.63276,      0.6342,      0.6355,     0.63679,     0.63762,     0.63918,     0.64055,     0.64164,     0.64308,     0.64429,     0.64603,     0.64729,\n",
       "            0.64894,     0.65042,      0.6518,     0.65299,     0.65404,     0.65537,     0.65642,     0.65776,     0.65858,     0.65981,     0.66098,     0.66184,     0.66264,      0.6639,     0.66511,     0.66634,     0.66766,     0.66858,     0.66988,     0.67055,     0.67158,      0.6726,     0.67403,\n",
       "              0.675,     0.67578,      0.6764,     0.67727,     0.67794,     0.67887,     0.67959,     0.68051,      0.6814,     0.68167,     0.68232,     0.68322,      0.6841,     0.68535,     0.68608,     0.68654,     0.68688,     0.68755,      0.6882,     0.68966,     0.69047,     0.69137,     0.69213,\n",
       "            0.69288,     0.69382,     0.69481,      0.6954,     0.69585,     0.69636,     0.69672,     0.69748,     0.69831,     0.69888,     0.69939,     0.70024,     0.70129,      0.7022,     0.70297,     0.70376,     0.70448,     0.70495,     0.70575,     0.70626,     0.70682,     0.70758,      0.7087,\n",
       "            0.70917,     0.70964,      0.7101,     0.71107,     0.71183,     0.71204,     0.71228,     0.71286,     0.71333,      0.7142,     0.71475,     0.71538,     0.71564,     0.71625,     0.71708,     0.71739,     0.71781,     0.71825,      0.7188,      0.7193,     0.71962,     0.72024,     0.72096,\n",
       "            0.72141,     0.72198,     0.72271,     0.72334,      0.7239,     0.72455,     0.72538,     0.72559,     0.72607,     0.72689,     0.72736,     0.72767,     0.72814,     0.72857,       0.729,     0.72896,     0.72927,     0.73056,     0.73067,     0.73079,       0.731,     0.73199,     0.73231,\n",
       "            0.73322,     0.73387,      0.7341,     0.73435,     0.73486,     0.73552,     0.73605,     0.73641,     0.73644,     0.73709,     0.73723,     0.73766,     0.73786,     0.73883,     0.73929,     0.73988,     0.74033,     0.74076,     0.74131,     0.74206,     0.74244,     0.74327,     0.74371,\n",
       "            0.74464,     0.74494,     0.74521,      0.7459,     0.74631,     0.74656,     0.74683,     0.74711,      0.7472,     0.74734,     0.74761,     0.74792,     0.74884,     0.74902,     0.74955,     0.75016,     0.75045,     0.75073,     0.75082,     0.75107,     0.75136,     0.75154,     0.75177,\n",
       "            0.75192,      0.7522,     0.75281,     0.75334,     0.75363,     0.75394,     0.75475,     0.75484,     0.75505,      0.7553,     0.75577,     0.75595,     0.75642,     0.75666,     0.75714,     0.75771,     0.75802,     0.75822,     0.75873,      0.7591,     0.75979,     0.76027,     0.76117,\n",
       "            0.76133,     0.76172,     0.76185,     0.76244,      0.7628,     0.76295,     0.76358,     0.76381,     0.76406,     0.76421,     0.76425,     0.76437,     0.76462,     0.76457,     0.76485,     0.76508,     0.76558,     0.76581,     0.76642,     0.76639,     0.76656,     0.76672,     0.76702,\n",
       "             0.7676,     0.76774,     0.76831,     0.76853,     0.76905,     0.76923,     0.76938,     0.76946,     0.76966,     0.77044,     0.77104,     0.77111,     0.77161,       0.772,     0.77217,     0.77238,     0.77252,     0.77282,     0.77319,     0.77333,     0.77395,     0.77422,     0.77461,\n",
       "            0.77493,     0.77508,     0.77599,     0.77629,     0.77648,     0.77668,     0.77662,     0.77683,     0.77697,     0.77755,     0.77778,     0.77832,      0.7784,     0.77844,     0.77872,     0.77907,      0.7792,     0.77956,     0.77985,     0.78012,     0.78042,     0.78077,     0.78085,\n",
       "            0.78097,     0.78129,     0.78126,     0.78132,     0.78141,     0.78235,     0.78285,     0.78289,     0.78303,     0.78296,     0.78341,     0.78393,     0.78431,     0.78451,     0.78444,     0.78494,     0.78517,     0.78537,     0.78576,     0.78596,     0.78629,     0.78651,     0.78695,\n",
       "            0.78718,     0.78748,      0.7879,     0.78807,     0.78836,     0.78877,     0.78909,     0.78946,     0.78963,     0.78973,     0.78984,     0.79029,     0.79028,     0.79045,     0.79058,     0.79062,     0.79091,     0.79089,     0.79113,     0.79141,     0.79159,     0.79193,     0.79253,\n",
       "            0.79293,     0.79293,     0.79298,     0.79358,     0.79375,     0.79415,     0.79434,     0.79443,     0.79484,     0.79484,     0.79522,     0.79552,     0.79561,     0.79571,     0.79611,     0.79625,      0.7967,       0.797,      0.7971,     0.79712,     0.79712,     0.79727,     0.79746,\n",
       "            0.79747,     0.79767,     0.79764,     0.79791,       0.798,     0.79802,     0.79848,      0.7983,     0.79833,     0.79908,     0.79941,     0.79932,      0.7994,     0.79928,     0.79967,     0.80048,     0.80047,     0.80037,     0.80062,     0.80064,     0.80096,     0.80136,      0.8016,\n",
       "            0.80164,      0.8018,     0.80165,     0.80192,     0.80228,     0.80236,     0.80275,     0.80303,      0.8035,     0.80315,      0.8037,      0.8039,      0.8043,     0.80453,     0.80453,     0.80443,     0.80455,     0.80452,     0.80466,     0.80462,     0.80518,      0.8051,     0.80513,\n",
       "            0.80534,     0.80516,     0.80531,     0.80567,      0.8058,     0.80592,     0.80644,     0.80672,     0.80677,     0.80627,     0.80618,     0.80584,      0.8062,     0.80624,     0.80656,     0.80665,     0.80693,     0.80674,     0.80746,     0.80763,       0.808,     0.80856,     0.80849,\n",
       "            0.80881,     0.80863,     0.80855,     0.80868,     0.80924,     0.80941,      0.8095,     0.80986,      0.8099,        0.81,     0.81004,     0.80985,     0.80989,     0.80986,     0.81027,     0.81056,     0.81077,      0.8106,     0.81039,     0.81048,     0.81048,     0.81089,     0.81126,\n",
       "            0.81162,     0.81156,     0.81161,     0.81182,     0.81216,     0.81265,      0.8127,     0.81262,     0.81254,      0.8125,      0.8123,     0.81254,     0.81223,     0.81183,     0.81205,     0.81194,     0.81179,     0.81163,     0.81182,     0.81166,     0.81174,     0.81163,     0.81168,\n",
       "            0.81177,     0.81182,     0.81187,     0.81174,     0.81162,     0.81137,      0.8113,     0.81118,     0.81114,     0.81107,     0.81112,      0.8109,     0.81107,     0.81119,     0.81158,     0.81122,     0.81106,     0.81081,     0.81041,     0.81031,     0.81044,      0.8105,      0.8105,\n",
       "            0.81089,      0.8108,     0.81093,     0.81144,     0.81176,     0.81219,     0.81233,     0.81243,     0.81219,     0.81212,     0.81184,     0.81172,     0.81181,     0.81204,     0.81233,     0.81269,     0.81287,     0.81254,     0.81235,     0.81214,     0.81156,      0.8114,      0.8114,\n",
       "             0.8112,      0.8115,     0.81159,     0.81155,     0.81142,      0.8111,     0.81065,     0.81061,     0.81091,     0.81062,     0.81055,     0.81038,     0.81008,     0.80992,     0.80915,     0.80941,     0.80903,     0.80891,     0.80849,     0.80863,     0.80828,     0.80836,     0.80846,\n",
       "            0.80846,     0.80802,     0.80803,     0.80793,     0.80797,     0.80816,     0.80808,     0.80852,     0.80848,     0.80821,     0.80804,     0.80752,      0.8066,     0.80612,     0.80586,     0.80552,     0.80552,     0.80473,     0.80407,     0.80304,     0.80268,     0.80243,     0.80132,\n",
       "            0.80084,     0.80043,     0.80005,     0.80046,      0.7997,     0.79978,     0.79926,     0.79946,     0.79874,     0.79869,     0.79811,     0.79831,     0.79815,     0.79821,      0.7982,      0.7982,      0.7982,     0.79807,     0.79753,     0.79626,     0.79553,     0.79494,     0.79455,\n",
       "            0.79426,     0.79404,     0.79349,     0.79357,     0.79363,     0.79265,      0.7919,     0.79128,      0.7908,     0.79061,     0.78961,     0.78891,      0.7885,     0.78794,     0.78642,     0.78592,      0.7857,      0.7852,     0.78434,     0.78433,      0.7844,     0.78464,     0.78415,\n",
       "             0.7835,     0.78267,     0.78223,     0.78185,     0.78128,     0.78032,     0.77992,      0.7794,     0.77863,     0.77771,     0.77726,     0.77623,     0.77531,     0.77395,     0.77254,      0.7717,     0.77116,      0.7706,     0.77022,     0.76976,     0.76803,     0.76726,     0.76702,\n",
       "            0.76602,      0.7651,     0.76425,     0.76251,     0.76201,     0.76142,      0.7609,     0.75988,     0.75845,     0.75777,     0.75576,     0.75494,      0.7528,     0.75193,     0.75033,     0.74721,     0.74523,     0.74388,     0.74342,      0.7427,      0.7406,     0.74031,     0.73833,\n",
       "            0.73777,     0.73676,      0.7326,     0.73178,     0.72971,     0.72877,     0.72481,     0.72282,     0.71867,     0.71778,     0.71466,     0.71427,     0.70997,     0.70759,      0.7068,     0.70187,     0.69894,     0.69798,     0.69561,     0.69176,     0.69085,     0.68702,     0.68607,\n",
       "            0.68374,     0.68034,     0.67928,     0.67434,     0.67007,      0.6653,     0.66447,     0.66032,     0.65465,     0.65405,     0.64809,     0.64258,     0.64103,     0.63578,     0.62807,     0.62739,     0.62292,     0.61616,     0.61559,     0.60706,     0.60069,     0.59887,     0.59336,\n",
       "            0.58876,     0.58206,     0.58088,     0.57574,     0.57181,     0.56278,      0.5619,     0.55666,     0.54947,     0.54855,     0.54134,     0.53516,     0.53416,      0.5251,     0.51933,       0.513,      0.5123,     0.50464,     0.49708,     0.49644,     0.48886,     0.48324,     0.47433,\n",
       "            0.46495,     0.46386,     0.45853,     0.45208,     0.44457,     0.44337,     0.43702,     0.43059,     0.42568,     0.42355,     0.41616,     0.40798,     0.39947,     0.39893,     0.38851,     0.38109,     0.37394,     0.36613,      0.3598,      0.3525,     0.35049,     0.34548,     0.33847,\n",
       "            0.33226,     0.32653,     0.32543,     0.31922,     0.31362,     0.30513,      0.3014,     0.29949,     0.29243,     0.28445,     0.27929,     0.27342,     0.26827,     0.26632,     0.26161,     0.25365,     0.25171,     0.24484,     0.23871,     0.23516,     0.22771,      0.2225,     0.21828,\n",
       "            0.21683,     0.20977,      0.2048,     0.19972,     0.19554,     0.19284,     0.18939,     0.18384,     0.17867,     0.17148,     0.17025,     0.16351,     0.15926,     0.15351,     0.14593,      0.1401,     0.13607,     0.13286,     0.13242,     0.12964,     0.12656,       0.124,     0.12103,\n",
       "            0.11656,     0.11359,     0.10935,     0.10832,     0.10528,     0.10336,     0.10068,    0.098014,    0.096083,    0.091076,    0.088745,    0.085694,    0.084472,     0.08057,    0.077823,    0.075851,    0.072306,    0.070337,    0.068743,    0.065975,    0.064769,    0.062104,    0.060765,\n",
       "           0.059553,    0.057542,    0.056325,    0.055086,    0.053899,    0.051073,    0.048642,     0.04703,    0.046185,    0.045598,    0.044983,    0.044509,    0.042467,    0.040905,    0.039588,    0.039058,    0.038235,    0.036272,    0.035364,    0.033564,    0.033285,    0.031042,    0.030039,\n",
       "           0.027966,    0.027123,    0.025661,    0.025239,    0.023372,    0.022506,    0.021285,    0.020861,     0.01967,    0.019626,    0.019343,    0.018659,    0.017963,     0.01775,    0.016104,    0.015708,    0.014398,    0.013611,    0.013105,    0.012763,    0.011811,    0.010943,   0.0098862,\n",
       "          0.0088352,    0.008692,   0.0085488,   0.0079726,   0.0076859,    0.007136,   0.0069639,   0.0067918,   0.0066196,   0.0064474,   0.0062956,   0.0061999,   0.0061042,   0.0060085,   0.0059128,   0.0053131,   0.0047038,   0.0042402,   0.0040819,    0.003938,   0.0037935,   0.0036208,   0.0034481,\n",
       "          0.0033304,   0.0032519,   0.0031733,   0.0030948,   0.0030162,   0.0029077,   0.0026916,   0.0023023,   0.0020272,   0.0019191,    0.001811,   0.0017029,   0.0016474,   0.0015993,   0.0015513,   0.0015032,   0.0014551,    0.001407,   0.0013589,   0.0013109,   0.0011837,  0.00076981,  0.00064609,\n",
       "         0.00052235,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.068402,    0.068402,    0.098746,     0.12281,     0.14251,     0.15996,     0.17513,     0.18813,     0.20006,     0.21102,     0.22136,     0.23155,     0.24075,     0.24851,     0.25682,     0.26426,     0.27156,     0.27878,     0.28513,     0.29173,     0.29773,     0.30341,     0.30903,\n",
       "            0.31401,     0.32005,     0.32468,      0.3302,     0.33496,     0.33958,     0.34428,     0.34845,     0.35297,     0.35722,      0.3609,     0.36498,     0.36843,     0.37208,      0.3757,      0.3801,     0.38369,     0.38713,     0.38965,     0.39263,     0.39643,      0.3997,     0.40206,\n",
       "            0.40504,     0.40706,     0.40999,     0.41329,     0.41582,     0.41759,     0.42024,      0.4225,     0.42514,     0.42737,     0.42964,      0.4314,     0.43334,     0.43561,     0.43765,     0.43975,     0.44181,      0.4441,     0.44638,     0.44796,     0.44953,     0.45147,     0.45357,\n",
       "            0.45528,     0.45702,     0.45925,     0.46108,     0.46336,     0.46536,     0.46711,     0.46804,     0.46936,     0.47173,     0.47348,     0.47518,      0.4768,     0.47827,     0.47978,     0.48084,     0.48279,     0.48443,     0.48587,     0.48758,     0.48909,     0.49109,     0.49261,\n",
       "            0.49453,     0.49624,     0.49786,     0.49925,     0.50053,     0.50215,     0.50345,     0.50509,     0.50617,     0.50776,     0.50914,     0.51016,     0.51118,     0.51268,     0.51413,     0.51567,     0.51734,     0.51859,     0.52016,     0.52111,     0.52242,     0.52365,     0.52545,\n",
       "            0.52663,     0.52772,     0.52853,     0.52961,     0.53049,     0.53163,     0.53265,     0.53385,     0.53508,     0.53552,     0.53649,     0.53768,     0.53884,     0.54039,     0.54144,     0.54208,     0.54258,     0.54348,     0.54444,     0.54627,     0.54729,     0.54849,     0.54946,\n",
       "            0.55047,      0.5518,     0.55313,     0.55395,      0.5546,     0.55533,     0.55592,      0.5569,     0.55796,     0.55876,     0.55949,     0.56065,       0.562,     0.56317,     0.56417,     0.56526,      0.5662,     0.56688,     0.56807,     0.56882,     0.56953,     0.57068,     0.57223,\n",
       "            0.57291,     0.57362,     0.57429,     0.57557,     0.57656,     0.57701,     0.57739,     0.57815,     0.57878,        0.58,     0.58073,     0.58157,     0.58191,     0.58288,     0.58397,     0.58453,     0.58511,      0.5857,     0.58659,     0.58743,     0.58786,     0.58869,     0.58965,\n",
       "            0.59026,     0.59102,     0.59199,     0.59294,     0.59368,     0.59455,     0.59567,     0.59597,     0.59669,     0.59781,     0.59853,     0.59913,     0.59976,     0.60035,     0.60112,      0.6013,     0.60193,     0.60368,     0.60394,     0.60427,     0.60456,     0.60599,     0.60644,\n",
       "            0.60769,     0.60858,     0.60908,     0.60943,     0.61013,     0.61104,     0.61176,     0.61227,      0.6124,      0.6133,     0.61358,     0.61437,     0.61465,       0.616,     0.61663,     0.61765,     0.61852,     0.61916,     0.61993,     0.62098,     0.62151,     0.62278,     0.62339,\n",
       "            0.62489,     0.62532,     0.62587,     0.62707,     0.62771,     0.62819,     0.62867,     0.62928,      0.6295,      0.6298,     0.63038,     0.63091,     0.63223,     0.63249,      0.6333,     0.63422,     0.63462,     0.63503,     0.63546,     0.63592,     0.63644,      0.6369,     0.63724,\n",
       "            0.63758,     0.63805,     0.63894,     0.63981,     0.64031,     0.64087,     0.64204,     0.64217,      0.6428,     0.64325,     0.64394,     0.64431,     0.64509,     0.64555,     0.64636,      0.6475,     0.64816,     0.64852,     0.64942,     0.64996,     0.65108,     0.65179,     0.65312,\n",
       "            0.65335,     0.65397,     0.65422,      0.6552,     0.65574,     0.65623,     0.65722,     0.65778,     0.65837,      0.6586,     0.65899,     0.65937,     0.65987,     0.66002,     0.66077,     0.66111,     0.66186,     0.66231,     0.66326,     0.66341,     0.66378,     0.66403,     0.66459,\n",
       "            0.66546,     0.66589,     0.66676,      0.6672,     0.66798,     0.66837,     0.66859,     0.66883,     0.66926,     0.67066,     0.67157,     0.67198,     0.67291,      0.6735,     0.67388,     0.67431,     0.67453,     0.67511,     0.67566,     0.67611,     0.67718,     0.67772,     0.67832,\n",
       "            0.67887,     0.67916,     0.68067,     0.68114,     0.68155,     0.68185,     0.68194,     0.68234,     0.68279,     0.68369,     0.68404,     0.68499,     0.68513,     0.68531,     0.68598,      0.6867,      0.6871,     0.68766,     0.68811,     0.68875,     0.68925,     0.68992,     0.69005,\n",
       "            0.69023,     0.69085,     0.69106,      0.6912,     0.69154,     0.69327,     0.69432,     0.69449,     0.69485,     0.69512,     0.69582,     0.69678,     0.69742,     0.69808,     0.69822,     0.69928,      0.6999,     0.70035,      0.7011,     0.70142,     0.70193,     0.70229,     0.70326,\n",
       "            0.70362,      0.7041,      0.7049,     0.70521,     0.70591,     0.70669,     0.70734,     0.70794,     0.70839,     0.70913,     0.70935,     0.71007,      0.7106,     0.71087,     0.71117,     0.71168,     0.71244,     0.71268,     0.71306,     0.71356,       0.714,     0.71463,     0.71575,\n",
       "            0.71655,     0.71682,     0.71731,     0.71829,     0.71862,     0.71951,     0.71996,     0.72038,     0.72119,     0.72148,     0.72247,     0.72331,     0.72346,     0.72377,     0.72442,     0.72477,     0.72583,     0.72662,     0.72678,     0.72696,     0.72725,      0.7275,     0.72825,\n",
       "            0.72856,     0.72904,     0.72912,     0.72972,     0.73027,     0.73064,     0.73141,     0.73165,     0.73175,     0.73316,     0.73387,     0.73385,     0.73416,     0.73423,     0.73488,     0.73641,     0.73688,     0.73696,     0.73744,     0.73788,     0.73841,     0.73909,     0.73951,\n",
       "            0.73973,      0.7403,     0.74065,     0.74127,     0.74202,     0.74247,     0.74314,     0.74408,      0.7452,     0.74537,     0.74632,     0.74714,     0.74782,     0.74823,     0.74854,     0.74882,     0.74919,     0.74961,     0.75033,     0.75042,     0.75139,     0.75158,     0.75186,\n",
       "            0.75232,     0.75241,     0.75274,     0.75353,     0.75391,     0.75428,     0.75535,     0.75601,     0.75658,     0.75664,     0.75669,     0.75689,     0.75769,     0.75793,      0.7585,     0.75898,     0.75964,      0.7598,     0.76108,     0.76171,      0.7632,     0.76421,     0.76442,\n",
       "              0.765,      0.7655,      0.7657,      0.7661,     0.76712,     0.76776,     0.76826,     0.76907,     0.76915,      0.7695,     0.76975,     0.76991,     0.77017,     0.77063,     0.77137,     0.77207,     0.77262,     0.77309,     0.77331,     0.77376,     0.77383,     0.77458,     0.77543,\n",
       "            0.77621,     0.77651,     0.77677,     0.77733,     0.77846,     0.77958,     0.77999,     0.78021,     0.78042,     0.78053,     0.78071,     0.78152,     0.78203,     0.78202,     0.78297,     0.78331,     0.78376,     0.78421,     0.78477,     0.78483,     0.78551,     0.78586,     0.78651,\n",
       "            0.78706,     0.78772,     0.78799,     0.78855,     0.78889,     0.78881,     0.78943,     0.79009,     0.79028,     0.79044,     0.79112,     0.79137,     0.79208,     0.79259,     0.79383,      0.7941,     0.79458,      0.7949,     0.79529,     0.79576,      0.7962,     0.79707,     0.79731,\n",
       "            0.79825,     0.79865,      0.7991,      0.8005,     0.80114,     0.80259,     0.80347,     0.80447,     0.80482,      0.8055,     0.80597,     0.80636,     0.80686,     0.80759,     0.80873,     0.80951,     0.81029,     0.81047,     0.81088,      0.8111,      0.8112,     0.81156,     0.81216,\n",
       "            0.81282,     0.81365,     0.81426,     0.81483,     0.81479,     0.81587,     0.81603,     0.81639,     0.81714,     0.81765,     0.81847,     0.81892,     0.81898,     0.81953,     0.81975,     0.82055,     0.82089,     0.82196,     0.82198,     0.82277,     0.82361,      0.8255,     0.82606,\n",
       "            0.82654,     0.82712,     0.82759,     0.82796,     0.82829,     0.82914,     0.82982,     0.83123,     0.83234,     0.83374,     0.83415,     0.83449,     0.83423,     0.83442,     0.83483,     0.83555,     0.83605,     0.83632,     0.83677,       0.837,     0.83723,     0.83817,     0.83853,\n",
       "            0.83874,     0.83913,     0.83954,     0.84049,     0.84056,     0.84116,     0.84217,     0.84331,     0.84381,     0.84428,     0.84451,     0.84614,     0.84704,     0.84753,     0.84795,     0.84913,     0.84982,     0.85034,      0.8502,     0.85167,     0.85166,     0.85223,     0.85305,\n",
       "            0.85389,     0.85487,     0.85529,     0.85662,     0.85782,     0.85874,     0.85989,     0.86025,     0.86076,       0.862,     0.86233,     0.86274,      0.8636,     0.86463,     0.86506,     0.86523,     0.86607,     0.86685,     0.86815,     0.86876,     0.86896,     0.87046,     0.87092,\n",
       "            0.87112,     0.87119,      0.8717,     0.87284,     0.87271,      0.8729,     0.87363,     0.87435,     0.87438,     0.87438,     0.87638,     0.87678,     0.87678,     0.87754,     0.87765,     0.87856,     0.87929,     0.87952,     0.88083,     0.88139,     0.88233,      0.8831,     0.88479,\n",
       "             0.8852,     0.88596,     0.88623,     0.88754,     0.88782,     0.88861,      0.8899,     0.89015,     0.89161,     0.89183,     0.89308,     0.89315,     0.89347,     0.89447,     0.89439,     0.89425,     0.89434,     0.89555,     0.89646,     0.89731,     0.89993,     0.90038,     0.90077,\n",
       "            0.90191,     0.90279,     0.90305,     0.90395,     0.90409,     0.90445,     0.90438,     0.90604,      0.9072,     0.90734,     0.90756,      0.9085,     0.90928,     0.90938,      0.9101,     0.91037,     0.91131,     0.91202,     0.91302,     0.91451,     0.91557,     0.91491,     0.91505,\n",
       "            0.91713,     0.91781,     0.91826,     0.91838,     0.91928,     0.92012,     0.92083,     0.92128,     0.92136,     0.92238,     0.92165,     0.92143,     0.92187,     0.92207,      0.9215,     0.92232,     0.92283,     0.92394,     0.92423,     0.92353,     0.92406,     0.92376,     0.92482,\n",
       "            0.92485,     0.92495,     0.92498,     0.92639,     0.92701,     0.92767,     0.92752,     0.92753,     0.92631,     0.92615,     0.92777,     0.92848,     0.92925,     0.92867,     0.92866,     0.93006,     0.93045,     0.93016,     0.93094,     0.93136,     0.93112,     0.93232,     0.93302,\n",
       "            0.93486,     0.93467,     0.93554,     0.93503,     0.93494,     0.93598,     0.93514,     0.93499,     0.93806,     0.93769,     0.93818,     0.93836,     0.93898,     0.93953,     0.93771,     0.93706,     0.93719,     0.93649,      0.9385,     0.94128,      0.9409,     0.94166,     0.94029,\n",
       "            0.93994,     0.93969,     0.93983,     0.94006,     0.94082,     0.94101,     0.94227,     0.94186,     0.94105,     0.94553,     0.94514,     0.94501,     0.94505,     0.94703,     0.94722,     0.94543,     0.94497,     0.94999,     0.94999,     0.95485,     0.95325,     0.95359,     0.95448,\n",
       "            0.95541,     0.95381,     0.95763,     0.95821,     0.95726,     0.95663,     0.95579,     0.95438,     0.95689,     0.95704,     0.95672,     0.95485,     0.95581,      0.9541,     0.95164,     0.94958,     0.94805,     0.95209,     0.95193,     0.95471,     0.95815,     0.96313,     0.96524,\n",
       "            0.96704,     0.97273,      0.9775,     0.97836,     0.97771,     0.97729,     0.97666,     0.97992,     0.97951,     0.97835,     0.97777,     0.97761,      0.9812,     0.98027,     0.97956,     0.97902,     0.97797,     0.98291,     0.98251,     0.98776,     0.98753,     0.98699,     0.98669,\n",
       "            0.98642,     0.98594,     0.98563,      0.9853,     0.99243,       0.992,     0.99159,      0.9913,     0.99114,     0.99102,      0.9909,      0.9908,     0.99035,     0.98998,     0.98964,      0.9895,     0.98927,     0.98869,      0.9884,     0.98777,     0.98767,     0.98677,     0.98633,\n",
       "            0.98532,     0.98486,       0.984,     0.98373,     0.98244,     0.98177,     0.98073,     0.98034,     0.98224,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.98499,     0.98499,     0.98203,     0.98077,      0.9795,     0.97717,      0.9759,     0.97485,     0.97294,     0.97168,     0.97125,     0.96999,     0.96956,     0.96914,     0.96893,     0.96872,     0.96829,     0.96766,     0.96724,     0.96639,     0.96576,     0.96576,     0.96491,\n",
       "            0.96428,     0.96364,     0.96301,     0.96259,     0.96216,     0.96174,     0.96153,     0.96111,     0.96026,     0.95984,     0.95963,     0.95899,     0.95846,     0.95836,     0.95836,     0.95815,     0.95815,     0.95794,     0.95773,     0.95751,     0.95709,     0.95709,     0.95646,\n",
       "            0.95603,     0.95603,     0.95603,     0.95603,     0.95582,     0.95561,      0.9554,     0.95498,     0.95456,     0.95371,     0.95308,     0.95308,     0.95265,     0.95265,     0.95202,     0.95202,     0.95202,     0.95202,     0.95117,     0.95096,     0.95096,      0.9499,     0.94969,\n",
       "            0.94969,     0.94948,     0.94927,     0.94927,     0.94904,     0.94864,     0.94821,     0.94758,     0.94737,     0.94695,     0.94673,     0.94673,     0.94673,     0.94673,     0.94652,      0.9461,     0.94547,     0.94515,     0.94442,      0.9442,     0.94378,     0.94378,     0.94356,\n",
       "            0.94356,     0.94356,     0.94356,     0.94356,     0.94335,     0.94314,     0.94293,     0.94272,      0.9423,     0.94187,     0.94187,     0.94187,     0.94166,     0.94166,     0.94166,     0.94139,     0.94112,     0.94063,      0.9406,     0.94018,     0.93997,     0.93997,     0.93976,\n",
       "            0.93976,     0.93934,     0.93912,     0.93912,     0.93891,     0.93891,     0.93849,     0.93828,     0.93786,     0.93753,     0.93701,      0.9368,     0.93659,     0.93659,     0.93617,     0.93595,     0.93574,     0.93553,     0.93515,     0.93511,     0.93511,      0.9349,      0.9349,\n",
       "            0.93469,     0.93426,     0.93405,     0.93384,     0.93363,     0.93342,     0.93301,       0.933,       0.933,     0.93278,     0.93257,     0.93236,     0.93236,     0.93236,     0.93236,     0.93215,     0.93215,     0.93194,     0.93152,      0.9313,      0.9313,     0.93088,     0.93067,\n",
       "            0.93046,     0.93025,     0.93004,     0.93004,     0.93004,     0.92959,      0.9294,      0.9294,      0.9294,     0.92919,     0.92919,     0.92919,     0.92919,     0.92877,     0.92877,     0.92842,     0.92834,     0.92834,     0.92794,      0.9275,      0.9275,      0.9275,      0.9275,\n",
       "             0.9275,      0.9275,      0.9275,     0.92729,     0.92729,     0.92729,     0.92729,     0.92729,     0.92708,     0.92708,     0.92687,     0.92644,     0.92644,     0.92644,     0.92599,     0.92543,     0.92496,     0.92496,     0.92471,     0.92433,     0.92433,     0.92412,     0.92412,\n",
       "            0.92412,     0.92412,     0.92369,     0.92369,     0.92369,     0.92369,     0.92369,     0.92369,     0.92348,     0.92348,      0.9233,     0.92285,     0.92285,     0.92285,     0.92285,     0.92243,     0.92189,     0.92179,     0.92179,     0.92179,     0.92179,     0.92158,     0.92158,\n",
       "            0.92116,     0.92116,     0.92077,     0.92031,     0.92018,     0.91989,     0.91968,     0.91924,     0.91904,     0.91883,     0.91841,      0.9182,      0.9182,      0.9182,     0.91807,     0.91799,     0.91799,     0.91799,     0.91735,     0.91714,     0.91693,     0.91651,     0.91651,\n",
       "            0.91623,     0.91609,     0.91609,     0.91587,     0.91566,     0.91546,     0.91545,     0.91545,      0.9148,     0.91461,     0.91461,     0.91439,     0.91418,     0.91397,     0.91376,     0.91313,     0.91272,     0.91259,     0.91228,     0.91228,     0.91207,     0.91207,     0.91207,\n",
       "            0.91207,     0.91199,     0.91186,     0.91165,     0.91165,      0.9111,     0.91101,     0.91059,     0.91017,     0.91017,     0.90953,     0.90914,      0.9089,     0.90849,     0.90784,     0.90784,     0.90784,     0.90763,     0.90759,     0.90721,       0.907,       0.907,     0.90679,\n",
       "            0.90679,     0.90636,     0.90633,     0.90615,     0.90615,     0.90594,     0.90594,     0.90573,     0.90552,     0.90509,     0.90509,     0.90456,     0.90425,     0.90425,     0.90404,     0.90383,     0.90383,     0.90361,     0.90361,     0.90321,     0.90298,     0.90277,     0.90277,\n",
       "            0.90264,     0.90256,     0.90235,     0.90235,     0.90213,     0.90213,     0.90184,     0.90171,     0.90129,     0.90129,     0.90129,     0.90108,     0.90108,     0.90087,     0.90044,     0.90015,     0.89981,     0.89981,     0.89981,     0.89945,     0.89939,     0.89918,     0.89918,\n",
       "            0.89918,     0.89896,     0.89854,     0.89845,     0.89812,     0.89769,     0.89727,     0.89706,     0.89685,     0.89622,     0.89622,     0.89601,     0.89593,     0.89537,     0.89495,     0.89453,      0.8941,     0.89389,     0.89368,     0.89368,     0.89368,     0.89368,     0.89326,\n",
       "            0.89326,     0.89326,     0.89305,     0.89299,     0.89262,     0.89241,      0.8922,      0.8922,     0.89192,     0.89099,     0.89093,     0.89093,     0.89009,     0.89009,     0.88996,     0.88924,     0.88882,      0.8884,      0.8884,     0.88832,      0.8881,     0.88797,     0.88776,\n",
       "            0.88754,     0.88711,     0.88649,     0.88649,     0.88641,     0.88607,     0.88586,     0.88545,     0.88523,      0.8848,     0.88426,     0.88375,     0.88375,     0.88353,     0.88353,     0.88336,      0.8829,     0.88248,     0.88248,     0.88227,     0.88184,     0.88184,     0.88121,\n",
       "            0.88079,     0.88057,     0.88036,     0.88015,     0.87958,      0.8791,      0.8791,      0.8783,     0.87825,     0.87804,      0.8778,     0.87762,     0.87737,     0.87698,     0.87698,     0.87677,     0.87608,     0.87571,     0.87565,     0.87508,     0.87508,     0.87508,     0.87508,\n",
       "            0.87487,     0.87445,      0.8736,     0.87339,     0.87318,     0.87275,     0.87275,     0.87212,      0.8717,     0.87064,     0.87064,     0.87001,     0.87001,        0.87,     0.86958,     0.86895,     0.86874,      0.8681,     0.86747,     0.86726,     0.86726,     0.86684,     0.86652,\n",
       "            0.86641,     0.86588,     0.86578,     0.86557,     0.86536,     0.86514,     0.86493,     0.86472,     0.86409,     0.86288,     0.86261,     0.86155,     0.86134,     0.86113,     0.86113,     0.86071,     0.86049,     0.85986,     0.85986,     0.85944,     0.85838,     0.85838,     0.85796,\n",
       "            0.85796,      0.8569,     0.85648,     0.85627,     0.85627,     0.85584,     0.85542,     0.85521,     0.85521,       0.855,     0.85479,     0.85415,     0.85394,     0.85331,     0.85331,      0.8531,     0.85289,     0.85193,     0.85119,     0.85086,     0.85077,     0.85077,     0.85056,\n",
       "            0.85042,     0.84993,     0.84971,      0.8495,     0.84892,     0.84866,     0.84827,     0.84785,     0.84742,      0.8472,     0.84656,     0.84612,     0.84485,     0.84401,     0.84337,     0.84274,     0.84189,     0.84105,     0.84081,     0.84039,     0.83978,     0.83915,     0.83851,\n",
       "            0.83809,     0.83746,     0.83724,     0.83633,     0.83569,     0.83526,     0.83441,     0.83344,     0.83313,      0.8328,     0.83217,     0.83143,       0.831,     0.83069,     0.83014,     0.82908,     0.82823,     0.82738,     0.82611,     0.82541,      0.8252,      0.8244,     0.82414,\n",
       "            0.82393,     0.82333,     0.82311,     0.82268,     0.82266,     0.82202,     0.82139,     0.82055,      0.8197,     0.81885,      0.8178,     0.81716,     0.81682,     0.81653,     0.81597,      0.8159,     0.81547,     0.81463,     0.81383,     0.81319,     0.81192,     0.81124,     0.81064,\n",
       "            0.80958,     0.80936,     0.80893,     0.80829,     0.80808,     0.80638,     0.80533,      0.8049,     0.80478,     0.80372,     0.80279,     0.80202,     0.80138,     0.80053,     0.79883,     0.79856,     0.79751,     0.79628,     0.79543,     0.79497,     0.79351,     0.79192,     0.79159,\n",
       "            0.79116,     0.78979,     0.78936,     0.78884,     0.78863,     0.78821,     0.78744,     0.78701,     0.78595,     0.78419,     0.78351,     0.78224,     0.78075,     0.77969,     0.77884,     0.77756,     0.77714,     0.77544,     0.77383,     0.77173,     0.77087,     0.76961,     0.76728,\n",
       "            0.76622,     0.76515,     0.76411,     0.76406,     0.76263,     0.76228,     0.76052,     0.75994,     0.75825,     0.75777,     0.75654,     0.75559,      0.7546,     0.75431,     0.75396,     0.75303,     0.75248,     0.75185,     0.75101,     0.74762,     0.74635,     0.74486,     0.74357,\n",
       "            0.74242,     0.74128,     0.74001,     0.73917,     0.73838,       0.736,     0.73388,     0.73255,     0.73135,     0.73015,      0.7282,     0.72672,     0.72543,     0.72374,     0.72089,     0.71993,     0.71898,     0.71761,     0.71528,     0.71486,     0.71483,     0.71423,      0.7131,\n",
       "             0.7119,     0.71048,     0.70942,     0.70804,     0.70719,     0.70549,     0.70436,     0.70305,     0.70178,     0.70029,     0.69828,     0.69637,     0.69489,     0.69224,     0.68991,     0.68802,     0.68671,     0.68569,     0.68429,     0.68322,     0.67994,     0.67829,     0.67692,\n",
       "            0.67512,     0.67326,     0.67178,     0.66836,     0.66743,     0.66608,     0.66457,     0.66287,      0.6599,     0.65874,     0.65504,     0.65376,     0.65039,     0.64857,     0.64624,     0.64169,     0.63873,     0.63615,     0.63502,     0.63354,      0.6292,     0.62856,     0.62553,\n",
       "            0.62418,     0.62231,     0.61628,      0.6147,     0.61172,     0.61024,     0.60473,     0.60124,     0.59501,     0.59374,     0.58939,     0.58846,     0.58233,     0.57909,     0.57774,     0.57108,     0.56685,     0.56531,     0.56183,     0.55626,     0.55471,     0.55002,     0.54874,\n",
       "            0.54504,      0.5405,       0.539,     0.53276,     0.52716,     0.52101,     0.51976,     0.51456,     0.50769,     0.50666,     0.49975,      0.4933,     0.49135,     0.48515,     0.47638,     0.47538,     0.47013,      0.4622,     0.46148,     0.45212,     0.44497,     0.44305,     0.43681,\n",
       "            0.43184,     0.42464,     0.42338,     0.41766,      0.4134,     0.40391,     0.40303,     0.39766,     0.39057,     0.38968,     0.38216,     0.37592,     0.37481,     0.36603,     0.36045,     0.35418,     0.35346,     0.34624,     0.33906,     0.33841,     0.33143,     0.32614,       0.318,\n",
       "            0.30942,     0.30848,     0.30369,     0.29811,     0.29162,     0.29049,     0.28514,      0.2797,      0.2753,     0.27356,     0.26739,     0.26065,      0.2537,     0.25322,     0.24501,     0.23918,     0.23357,     0.22754,     0.22257,     0.21686,     0.21536,     0.21154,     0.20638,\n",
       "             0.2018,      0.1976,     0.19679,     0.19226,     0.18817,     0.18209,     0.17939,     0.17805,     0.17311,     0.16741,     0.16386,     0.15983,     0.15632,     0.15495,     0.15176,     0.14648,     0.14519,     0.14053,     0.13651,     0.13409,      0.1293,     0.12594,     0.12323,\n",
       "            0.12229,     0.11784,     0.11466,     0.11148,     0.10889,     0.10723,     0.10511,     0.10171,    0.098533,    0.094178,    0.093439,    0.089412,    0.086865,    0.083472,    0.079023,     0.07563,    0.073295,    0.071413,     0.07116,    0.069541,    0.067757,    0.066267,    0.064565,\n",
       "           0.062018,    0.060316,    0.057916,    0.057336,    0.055635,    0.054567,    0.053077,    0.051587,    0.050519,    0.047761,    0.046482,    0.044811,    0.044136,    0.042012,    0.040521,    0.039454,    0.037541,    0.036474,    0.035618,    0.034127,    0.033483,     0.03206,    0.031348,\n",
       "           0.030703,    0.029636,    0.028991,    0.028335,    0.027702,    0.026211,    0.024933,    0.024086,    0.023643,    0.023336,    0.023014,    0.022766,    0.021699,    0.020884,    0.020198,    0.019922,    0.019494,    0.018475,    0.018004,    0.017072,    0.016928,    0.015769,    0.015252,\n",
       "           0.014184,    0.013751,       0.013,    0.012783,    0.011827,    0.011383,    0.010759,    0.010543,   0.0099345,   0.0099101,   0.0097657,   0.0094176,   0.0090628,   0.0089545,   0.0081176,   0.0079162,    0.007251,   0.0068523,   0.0065957,   0.0064225,   0.0059404,   0.0055014,   0.0049677,\n",
       "          0.0044372,    0.004365,   0.0042927,   0.0040022,   0.0038578,   0.0035808,   0.0034941,   0.0034075,   0.0033208,   0.0032341,   0.0031577,   0.0031096,   0.0030614,   0.0030133,   0.0029652,   0.0026636,   0.0023575,   0.0021246,   0.0020451,   0.0019729,   0.0019004,   0.0018137,    0.001727,\n",
       "           0.001668,   0.0016286,   0.0015892,   0.0015498,   0.0015104,    0.001456,   0.0013476,   0.0011525,   0.0010146,  0.00096048,  0.00090632,  0.00085215,  0.00082438,   0.0008003,  0.00077623,  0.00075216,  0.00072808,  0.00070401,  0.00067993,  0.00065586,  0.00059222,  0.00038505,  0.00032315,\n",
       "         0.00026125,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.6119712877282402)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.58509])\n",
       "names: {0: 'person'}\n",
       "nt_per_class: array([4731])\n",
       "nt_per_image: array([1894])\n",
       "results_dict: {'metrics/precision(B)': np.float64(0.790444610156555), 'metrics/recall(B)': np.float64(0.8328049038258296), 'metrics/mAP50(B)': np.float64(0.8539104668565527), 'metrics/mAP50-95(B)': np.float64(0.5850891567139832), 'fitness': np.float64(0.6119712877282402)}\n",
       "save_dir: PosixPath('runs/detect/train5')\n",
       "speed: {'preprocess': 0.1303780553532177, 'inference': 3.516161205063569, 'loss': 0.002488568791833862, 'postprocess': 1.1637536557694559}\n",
       "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 6: Train the YOLOv8x model (super model with expensive infrastructure) ===\n",
    "\n",
    "# Load the YOLOv8x model (largest and most accurate version from Ultralytics)\n",
    "model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "# Train the model on your custom dataset with more compute resources\n",
    "model.train(\n",
    "    data=os.path.join(DATASET_PATH, \"people.yaml\").replace(\"\\\\\", \"/\"),  # Path to dataset config file\n",
    "    epochs=20,            # Train for more epochs to increase accuracy\n",
    "    imgsz=640,            # Input image size (standard for YOLOv8)\n",
    "    batch=16,             # Larger batch size, assuming enough GPU memory\n",
    "    cache=True,           # Cache dataset in memory to speed up training (uses more RAM)\n",
    "    device=0,             # Use GPU 0 for training (ensure CUDA is available)\n",
    "    workers=16,           # More data loading workers (useful for powerful CPUs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY5_r9u3FUEZ"
   },
   "source": [
    "## YOLOv8x Training Summary – High-Performance Model\n",
    "\n",
    "Training was conducted on **YOLOv8x.pt**, Ultralytics’ largest and most accurate object detection model, using the following configuration:\n",
    "\n",
    "### Environment & Setup\n",
    "\n",
    "- **Framework**: Ultralytics v8.3.168  \n",
    "- **Backend**: PyTorch 2.6.0 + CUDA 12.4  \n",
    "- **Device**: `NVIDIA A100-SXM4-40GB`\n",
    "- **Model**: `YOLOv8x` (68.1M params, 258.1 GFLOPs)  \n",
    "- **Dataset**: `people.yaml` – single class (\"person\")  \n",
    "- **Image Size**: 640x640  \n",
    "- **Epochs**: 20  \n",
    "- **Batch Size**: 16  \n",
    "- **Workers**: 16  \n",
    "- **Cache**: Enabled (fallback due to RAM limits)  \n",
    "- **Augmentations**: Blur, MedianBlur, ToGray, CLAHE  \n",
    "- **Optimizer**: `AdamW` (auto-selected)\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "| Metric              | Value   |\n",
    "|---------------------|---------|\n",
    "| **Precision**       | 0.790   |\n",
    "| **Recall**          | 0.833   |\n",
    "| **mAP@0.50**        | 0.854   |\n",
    "| **mAP@0.50:0.95**   | 0.585   |\n",
    "| **Fitness Score**   | 0.612   |\n",
    "\n",
    "- **Best model saved at**: `runs/detect/train5/weights/best.pt`  \n",
    "- **Validation Speed**: 3.5ms inference + 1.2ms postprocess/image  \n",
    "- **Training Time**: ~1.3 hours (20 epochs)\n",
    "\n",
    "---\n",
    "\n",
    "### Model Architecture (Simplified)\n",
    "\n",
    "- 209 layers  \n",
    "- 68,153,571 parameters  \n",
    "- 595 total modules  \n",
    "- Key blocks:\n",
    "  - Multiple `C2f` and `Conv` layers\n",
    "  - Upsampling + Concat for feature fusion\n",
    "  - Final `Detect` head with shape `[1, [320, 640, 640]]`  \n",
    "\n",
    "---\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The model consistently improved **mAP@50** from `0.54` (epoch 1) to `0.85` (epoch 20).\n",
    "- **mAP@50-95** reached `0.585`, indicating strong localisation across IoU thresholds.\n",
    "- **Memory** usage peaked at ~31.8GB on the A100 GPU.\n",
    "- The model handled 4731 validation instances across 1897 images.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **YOLOv8x** offers excellent performance with minimal tuning required.\n",
    "- Using **high compute resources** (A100 + fast storage), training was fast and accurate.\n",
    "- Ready for **production deployment** or **fine-tuning on edge tasks**.\n",
    "\n",
    "> Final model: `runs/detect/train5/weights/best.pt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFDWbzT6GRrb"
   },
   "source": [
    "### Downloading the Trained YOLOv8 Model (`best.pt`) from Colab\n",
    "\n",
    "After completing the training, the best weights are saved in:\n",
    "\n",
    "```php\n",
    "<runs/detect/<latest_run>/weights/best.pt>\n",
    "```\n",
    "\n",
    "\n",
    "The following code does three things:\n",
    "\n",
    "1. **Finds the latest training run folder** inside `runs/detect`.\n",
    "2. **Builds the full path** to the `best.pt` file (best-performing checkpoint).\n",
    "3. **Triggers the download** of this file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "rFMp_71z1VXY",
    "outputId": "66f455b8-977d-4a0d-f0fe-36cc7fb9020b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ac8699bc-8af2-408d-aafd-8750398ba5e2\", \"best.pt\", 136712755)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Identify the latest training run\n",
    "latest_run = sorted(os.listdir(\"runs/detect\"))[-1]\n",
    "\n",
    "# Construct full path to the best model weights\n",
    "model_path = f\"runs/detect/{latest_run}/weights/best.pt\"\n",
    "\n",
    "# Download the model file to your local machine\n",
    "files.download(model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
